title: "AnÃ¡lisis de respuestas binarias"
author: "MCP"
date: "19 de febrero de 2018"
output:
  pdf_document:
    includes:
      in_header: mystyles.sty
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## DistribuciÃ³n Binomial
Consideremos una variable aleatoria binomial que cuenta el nÃºmero de Ã©xitos de un experimento repetido $n=5$ veces, y supongamos que la probabilidad de Ã©xito es $\pi=0.6$. Se puede calcular la probabilidad de cada nÃºmero de Ã©xitos $w=0,1,2,3,4,5$. Por ejemplo, la probabilidad de $1$ Ã©xito en $5$ intentos es
$$ P(W=1)=\binom{5}{1}(0.6)^1 (1-0.6)^{5-1}=0.0768 $$
En R se usa la funciÃ³n "dbinom()"
```{r}
dbinom( x = 1 , size = 5, prob = 0.6 )
```
Podemos encontrar las probabilidades $w=0, \dots,5$ cambiando el argumento $x$ 
```{r}
dbinom(0:5, 5, 0.6)
```
Para representar los datos de manera m\'as descriptiva:
```{r}
prob <- dbinom( x = 0:5 , size = 5 , prob = 0.6)
prob_df <- data.frame( w = 0:5 , prob = round( x = prob , digits = 4) )
prob_df
```
Graficamos:
```{r}
pdf(file = "Figure1.1.pdf", width = 6, height = 6, colormodel = "cmyk") 
plot( x = prob_df$w , y = prob_df$prob , type = "h" , xlab = "w" , ylab ="P( W = w )" , main = 
        "GrÃ¡fica de una dist. binomial para n =5 , pi =0.6" , panel.first = grid( col = "gray"
  , lty = "dotted") , lwd = 3)
abline(h=0)
dev.off()
```
De forma alterna:
```{r}
plot(x = prob_df$w, y = prob_df$prob, type = "h", xlab = "w", ylab = "P(W=w)", main = 
       expression(paste("GrÃ¡fica de una distribuciÃ³n binomial para ", italic(n) == 5, " y ",
      italic(pi) == 0.6)), panel.first = grid(col="gray", lty="dotted"), lwd = 3)
```

## Â¿CuÃ¡les son nuestras hipÃ³tesis?

La distribuciÃ³n binomial es un modelo rasonable para la distribuciÃ³n de Ã©xitos en un nÃºmero dado de ensayos siempre y cuando se satisfagan ciertas condiciones, a saber:
\begin{enumerate}
  \item \emph{Hay $n$ ensayos id\'enticos}. \\
    La acci\'on que resulta en el ensayo y la medida tomada deben ser las mismas en cada     ensayo.
  \item \emph{Existen dos posibles resultados para cada ensayo}.
  \item \emph{Los ensayos son independientes unos de otros}.
    No existe factor alguno en la ejecuciÃ³n de los ensayos que pueda causar que un subconjunto de los ensayos se comporte de manera similar a otro. 
  \item \emph{La probabilidad de \'exito permanece constante para cada ensayo}.
  \item \emph{La variable aleatoria de inter\'es $W$ es el n\'umero de \'exitos}.
\end{enumerate}

## SimulaciÃ³n de una muestra binomial

Simularemos $1000$ osbervaciones aleatorias de $W$ a partir de una distribuciÃ³n binomial con $\pi = 0.6$ y $n=5$.
```{r}
set.seed(4848)
bin5<-rbinom(n = 1000, size = 5, prob = 0.6)
bin5[1:10]
```
En la teorÃ­a
$$ E(W)=n \pi = 5(0.6)=3  $$
$$ Var(W)=n \pi (1- \pi)=5(0.6)(0.4)=1.2  $$
Calculamos media y varianza muestrales:
```{r}
mean(bin5)
var(bin5)
```
Por supuesto, se esperarÃ­an valores mÃ¡s cercanos a los poblacionales con un tamaÃ±o de muestra mayor.

Para tratar de ver quÃ© tan bien la distribuciÃ³n observada sigue a la binomial, usamos "table()" para encontrar las frecuencias de cada posible respuesta y luego utilizamos "hist()" para graficar un histograma de frecuencias relativas.
```{r}
table(x = bin5)
# Histograma de frecuencias relativas
#hist(x = bin5, main = "Binomial con n=5, pi=0.6, 1000 observaciones", probability = TRUE,
     #ylab = "Relative frequency")  # La columna de la izquierda no se despliega correctamente
hist(x = bin5, main = "Binomial con n=5, pi=0.6, 1000 observaciones", probability = TRUE,
  breaks = c(-0.5:5.5), ylab = "Frecuencia relativa")
```

De otra manera
```{r}
save.count<-table(bin5)
save.count
barplot(height = save.count, names = c("0", "1", "2", "3", "4", "5"), main = "Binomial con
        n=5, pi=0.6, 1000 observaciones", xlab = "x")
```

## Inferencia para la probabilidad de Ã©xito

El objetivo es estimar y hacer inferencias acerca de la probabilidad del parÃ¡metro $\pi$ de la distribuciÃ³n de Bernoulli. 

\textbf{Estimaci\'on e inferencia de m\'axima verosimilitud  }

La funciÃ³n de verosimilitud es una funciÃ³n de uno o mÃ¡s parÃ¡metros condicionados a los datos observados. La funciÃ³n de verosimilitud para $\pi$ cuando $y_1, \cdots, y_n$ son observaciones de una distribuciÃ³n de Bernoulli es
$$ L(\pi \vert y_1, \cdots, y_n) = P(Y_1=y_1) \cdots P(Y_n=y_n) = \pi^{w}(1-\pi)^{n-w} $$

Cuando se registra el nÃºmero de Ã©xitos en un determinado nÃºmero de ensayos, la funciÃ³n de verosimilitud para $\pi$ es simplemente $L(\pi \vert w)=P(W=w)=\binom{n}{w}\pi^w(1-\pi)^{n-w}$. El valor de $\pi$ que maximiza la funciÃ³n de verosimilitud es considerado el valor mÃ¡s plausible para el parÃ¡metro y es llamado el estimador de mÃ¡xima verosimilitud (MLE en inglÃ©s).

En este caso, el MLE de $\pi$ es $\hat{\pi}=w/n$, la proporciÃ³n observada de Ã©xitos. Ya que $\bar{\pi}$ puede variar de muestra a muestra, es un estadÃ­stico y tiene su correspondiente distribuciÃ³n de probabilidad. Se puede mostrar que $\hat{\pi}$ tiene una distribuciÃ³n aproximadamente normal para muestras suficientemente grandes. La media es $\pi$ y la varianza se calcula:

\begin{align*}
\widehat{Var}(\hat{\pi}) & =\restr{-E \left\{ \dfrac{\partial^2 log [L(\pi \vert W)]}{\partial \pi^2}  \right\}^{-1}}{\pi = \hat{\pi}} \\
                     & = \restr{\left[ \dfrac{n}{\pi} - \dfrac{n}{1-\pi} \right]}{\pi = \hat{\pi}} \\
                     & = \dfrac{\hat{\pi}(1-\hat{\pi})}{n}
\end{align*}

NotaciÃ³n $\hat{\pi} \sim N(\pi,\widehat{Var}(\hat{\pi}))$.

\textbf{Intervalo de confianza de Wald}

Utilizando esta distribuciÃ³n normal, podemos tratar a $\dfrac{\hat{\pi}-\pi}{\sqrt{\widehat{Var}(\hat{\pi})}}$ como aproximadamente normal. Por ello, para $0<\alpha<1$ se tiene
$$P \left( Z_{\alpha/2} < \dfrac{\hat{\pi}-\pi}{\sqrt{\widehat{Var}(\hat{\pi})}} < Z_{1- \alpha/2} \right) \approx 1- \alpha$$



donde $Z_{\alpha}$ es el $\alpha$-Ã©simo cuantil de una distribuciÃ³n normal estÃ¡ndar. Reorganizando tÃ©rminos:
$$ P \left( \hat{\pi} - Z_{1- \alpha/2} \sqrt{\widehat{Var}(\hat{\pi})} < \pi < \hat{\pi} + Z_{1- \alpha/2} \sqrt{\widehat{Var}(\hat{\pi})}  \right) \approx 1 - \alpha  $$
Entonces, ahora tenemos una probabilidad aproximada que tiene el parÃ¡metro $\pi$ centrado entre dos estadÃ­sticos. Cuando se reemplazan $\hat{\pi}$ y $\widehat{Var}(\hat{\pi})$ con los valores observados de la muestra, se obtiene un intervalo de confianza del $(1- \alpha)100\%$ para $\pi$
$$ \hat{\pi}-Z_{1- \alpha/2}\sqrt{\hat{\pi}(1- \hat{\pi})/n} < \pi < \hat{\pi}+Z_{1- \alpha/2}\sqrt{\hat{\pi}(1- \hat{\pi})/n} $$
Los intervalos de confianza basados en la aproximaciÃ³n a la normal de los MLE's son llamados "Intervalos de confianza de Wald".

Cuando $w$ estÃ¡ cerca de $0$ o $n$, ocurren dos problemas:
\begin{enumerate}
  \item Los lÃ­mites calculados podrÃ­an ser menores a $0$ o mayores a $1$.
  \item Cuando $w$ es $0$ o $1$, $\sqrt{\hat{\pi}(1-\hat{\pi})}=0$ para $n>0$. Esto implica que los lÃ­mites inferior y superior son iguales.
\end{enumerate}

Supongamos que $w=4$ Ã©xitos son observados en $n=10$ ensayos. El intervalo de Wald para $\pi$ es $0.0964 < \pi < 0.7036$.
```{r}
w<-4
n<-10
alpha<-0.05
pi.hat<-w/n
var.wald<-pi.hat*(1-pi.hat)/n
lower<-pi.hat - qnorm(p = 1-alpha/2) * sqrt(var.wald)
upper<-pi.hat + qnorm(p = 1-alpha/2) * sqrt(var.wald)
round(data.frame(lower, upper), 4)
```
O bien
```{r}
round(pi.hat + qnorm(p = c(alpha/2, 1-alpha/2)) * sqrt(var.wald),4)
```
Este intervalo es algo extenso, sin embargo da informaciÃ³n de un rango para $\pi$ que posiblemente sea Ãºtil en prueba de hipÃ³tesis. Por ejemplo, una prueba de $H_0:\pi=0.5$ contra $H_a:\pi \neq 0.5$ no rechazarÃ­a $H_0$ puesto que $0.5$ se encuentra en este rango. Pero si la prueba fuera $H_0:\pi = 0.8$ contra $H_a:\pi \neq 0.8$, hay evidencia para rechazar la hipÃ³tesis nula.


\textbf{Intervalo de confianza de Wilson}:

Cuando $n<40$ se suele recomendar usar el intervalo de Wilson, el cual se obtiene a partir del estadÃ­stico de prueba 
$$ Z_0 = \cfrac{\hat{\pi}-\pi_0}{\sqrt{\pi_0(1-\pi_0)/n}},$$


el cual es llamado \emph{estad\'istico de prueba score}; se utiliza frecuentemente en la prueba de $H_0: \pi=\pi_0$ contra $H_a:\pi \neq \pi_0$, donde $0<\pi_0 <1$. Se puede aproximar la distribuciÃ³n de $Z_0$ a la distribuciÃ³n normal estÃ¡ndar para obtener $P(-Z_{1- \alpha/2}<Z_0<Z_{1-\alpha/2}) \approx 1 - \alpha$. Ya que el intervalo de Wilson estÃ¡ basado en una prueba "score", comunmente es referido por \emph{intervalo score}.

El intervalo de Wilson de $(1-\alpha)100\%$ es
$$ \tilde{\pi} \pm \dfrac{Z_{1-\alpha}\sqrt{n}}{n+Z_{1-\alpha/2}^2} \sqrt{\hat{\pi}(1-\hat{\pi})+\cfrac{Z_{1-\alpha/2}^2}{4n}},$$
donde 
$$ \tilde{\pi}=\cfrac{w+Z_{1-\alpha/2}^2/2}{n+Z_{1-\alpha/2}^2}$$
ObsÃ©rvese que el intervalo de Wilson siempre tiene lÃ­mites entre $0$ y $1$.


\textbf{Intervalo de confianza de Agresti-Coull}:

Este intervalo es recomendado cuando $n \geq 40$, la fÃ³rmula estÃ¡ dada por:
$$ \tilde{\pi}-Z_{1-\alpha/2} \sqrt{\cfrac{\tilde{\pi}(1-\tilde{\pi})}{n+Z_{1-\alpha/2}^2}}< \pi <  \tilde{\pi}+Z_{1-\alpha/2} \sqrt{\cfrac{\tilde{\pi}(1-\tilde{\pi})}{n+Z_{1-\alpha/2}^2}}$$

Supongamos tambiÃ©n que $w=4$ Ã©xitos son observados en $n=10$ ensayos. En R:
```{r}
p.tilde<-(w + qnorm(p = 1-alpha/2)^2 /2) / (n + qnorm(p = 1-alpha/2)^2)
p.tilde

# Intervalo de Wilson
round(p.tilde + qnorm(p = c(alpha/2, 1-alpha/2)) * sqrt(n) / (n+qnorm(p = 1-alpha/2)^2) *
        sqrt(pi.hat*(1-pi.hat) + qnorm(p = 1-alpha/2)^2/(4*n)),4)

# Intervalo de Agresti-Coull
var.ac<-p.tilde*(1-p.tilde) / (n+qnorm(p = 1-alpha/2)^2)
round(p.tilde + qnorm(p = c(alpha/2, 1-alpha/2)) * sqrt(var.ac),4)

# qnorm(p=c(alpha/2,1-alpha/2))
```

Se puede simplificar todavÃ­a mÃ¡s el trabajo haciendo uso de la paqueterÃ­a "binom":
```{r}
library(binom)
binom.confint(x = w, n = n, conf.level = 1-alpha, methods = "all")

```
TambiÃ©n se pueden obtener los intervalos uno a la vez y guardarlos en objetos
```{r}
# Intervalo Agresti-Coull
save.ci<-binom.confint(x = w, n = n, conf.level = 1-alpha, methods = "ac")
save.ci
```

